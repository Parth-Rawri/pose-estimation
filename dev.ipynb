{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5df24-db32-43ce-8f6c-f80610e98033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def transform_is_valid(transform, tolerance=1e-3):\n",
    "    \"\"\"\n",
    "    checks if transformation is a valid SE(3)\n",
    "    args:\n",
    "        transform (numpy.array [4,4])\n",
    "        tolerance (float, optional) \n",
    "    returns: \n",
    "        bool: True is `transform` is valid else False\n",
    "    \"\"\"\n",
    "    shape = transform.shape == (4,4)\n",
    "    isreal = np.all(np.isreal(transform))\n",
    "    last_row = np.allclose(transform[3, :], [0, 0, 0, 1], atol=tolerance)\n",
    "    R, T = transform[:3, :3], transform[:3, -1]\n",
    "    RtR = R.transpose() @ R\n",
    "    RRt = R @ R.transpose()\n",
    "    det = np.linalg.det(R)\n",
    "    valid = isreal and shape and np.allclose(RtR, np.eye(3), atol=tolerance) and np.allclose(RRt, np.eye(3), atol=tolerance) and np.isclose(det, 1, atol=tolerance)\n",
    "    return valid\n",
    "\n",
    "\n",
    "def camera_to_image(K, points):\n",
    "    \"\"\"\n",
    "    project points from camera coordinate system to digital image plane\n",
    "    args:\n",
    "        K (numpy.array [3, 3])\n",
    "        points [n, 3]\n",
    "    returns:\n",
    "        numpy.array [n, 2]: n 2D projections of the input points on the image plane\n",
    "    \"\"\"\n",
    "    fx, fy = K[0,0], K[1,1] \n",
    "    cx, cy = K[0,2], K[1,2]\n",
    "    x, y, z = points[:,0], points[:,1], points[:,2]\n",
    "    u, v = (np.round((fx * (x/z) + cx))).astype(int), (np.round((fy * (y/z) + cy))).astype(int)\n",
    "    points = np.vstack((u, v)).T\n",
    "    return points\n",
    "\n",
    "\n",
    "def world_to_camera(camera_pose, world_points):\n",
    "    \"\"\"\n",
    "    Transform the points in the world frame to the camera frame, using camera pose.\n",
    "    args:\n",
    "        camera_pose (numpy.array [4, 4])\n",
    "        points [n, 3]\n",
    "    returns:\n",
    "        numpy.array [n, 3]\n",
    "    \"\"\"\n",
    "    world_coords_h = np.hstack((world_points, np.ones((world_points.shape[0], 1), dtype=np.float32)))\n",
    "    camera_pose = np.linalg.inv(camera_pose)\n",
    "    camera_coords_h = (camera_pose @ world_coords_h.T).T\n",
    "    camera_coords_c = camera_coords_h[:, :3]\n",
    "    return camera_coords_c\n",
    "\n",
    "\n",
    "def depth_to_point_cloud(K, depth_image):\n",
    "    \"\"\"\n",
    "    back project a depth image to a point cloud\n",
    "    args:\n",
    "        K (numpy.array [3, 3])\n",
    "        depth_image (numpy.array [h, w]): each entry is a z depth value\n",
    "    Returns:\n",
    "        numpy.array [n, 3]: each row represents a different valid 3D point\n",
    "    \"\"\"\n",
    "    fx, fy = K[0,0], K[1,1] \n",
    "    cx, cy = K[0,2], K[1,2]\n",
    "    h, w = depth_image.shape\n",
    "    u, v = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    u, v = u.flatten(u), u.flatten(v)\n",
    "    Z = depth_image.flatten()\n",
    "    valid = Z > 0\n",
    "    u, v, Z = u[valid], v[valid], Z[valid]\n",
    "    X, Y = (u - cx) * Z/fx, (v - cy) * Z/fy\n",
    "    points = np.vstack((X, Y, Z)).T\n",
    "    return points    \n",
    "\n",
    "\n",
    "# test_rgbd_image_to_points(...) that will read an RGB-D image and convert it into point cloud and save it as test.ply.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723deaa-f905-4c08-820f-1da8de27d275",
   "metadata": {},
   "source": [
    ".copy(order='C'): Ensures the array is stored in C-contiguous order, which is important for performance in certain NumPy operations.\n",
    "astype(int): Converts the voxel bounds into integer values because the number of voxels must be a whole number.\n",
    "\n",
    "Use **stack** when you want to add a new dimension.\n",
    "Use **concatenate** when you want to merge along an existing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daace64a-4a0a-4e41-a1a5-5ab00cd3e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsdf.py\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "\n",
    "class TSDFVolume:\n",
    "    def __init__(self, volume_bounds, voxel_size):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            volume_bounds (numpy.array [3, 2]): rows index [x, y, z] and cols index [min_bound, max_bound]. Units are in meters.\n",
    "            voxel_size (float): The side length of each voxel in meters.\n",
    "        \"\"\"\n",
    "        # raise value error if not conformant\n",
    "        if volume_bounds.shape != (3,2):\n",
    "            raise ValueError(\"Incorrect Shape\")\n",
    "        if voxel_size <= 0.0:\n",
    "            raise ValueError(\"Invalid Size\")\n",
    "            \n",
    "        \"\"\"Numpy array [3, 2] of float32s, where rows index [x, y, z] and cols index [min_bound, max_bound]. Units are in meters.\"\"\"\n",
    "        self._volume_bounds = volume_bounds\n",
    "        \n",
    "        \"\"\"float side length in meters of each 3D voxel cube.\"\"\"\n",
    "        self._voxel_size = float(voxel_size)\n",
    "        \n",
    "        \"\"\"float tsdf truncation margin, the max allowable distance away from a surface in meters.\"\"\"\n",
    "        self._truncation_margin = 2 * self._voxel_size\n",
    "        # adjust volume bounds\n",
    "        self.voxel_bounds = np.ceil((self._volume_bounds[:,1]-self._volume_bounds[:,0])/self._voxel_size).copy(order='C').astype(int) \n",
    "        self._volume_bounds[:, 1] = self._volume_bounds[:, 0] + (self.voxel_bounds * self._voxel_size)\n",
    "        \n",
    "        \"\"\"Origin of the voxel grid in world coordinate. Units are in meters.\"\"\"\n",
    "        self._volume_origin = self._volume_bounds[:,0].copy(order='C').astype(np.float32)\n",
    "        print(f'Voxel volume size: {self.voxel_bounds[0]} x {self.voxel_bounds[1]} x {self.voxel_bounds[2]} - # voxels: {self.voxel_bounds[0] * self.voxel_bounds[1] * self.voxel_bounds[2]}')\n",
    "        \n",
    "        \"\"\"Numpy array of float32s representing tsdf volume where each voxel represents a volume self._voxel_size^3. Shape of this volume is determined by (max_bound - min_bound)/ self._voxel_size. Each entry contains the distance to the nearest surface in meters, truncated by self._truncation_margin.\"\"\"\n",
    "        self._tsdf_volume = np.ones(self.voxel_bounds).astype(np.float32)\n",
    "        \n",
    "        \"\"\"Numpy array of float32s with shape [self._tsdf_volume.shape, 3] in range [0.0, 255.0]. So each entry in the volume contains the average r, g, b color.\"\"\"\n",
    "        self._color_volume = np.zeros(np.append(self.voxel_bounds, 3)).astype(np.float32)\n",
    "        \n",
    "        \"\"\"Numpy array [number of voxels, 3] of uint8s. [[0, 0, 0], [0, 0, 1], ...,]. When a new observation is made, we need to determine which of these voxel coordinates is \"valid\" so we can decide what voxels to update.\"\"\"\n",
    "        xv, yv, zv = np.meshgrid(\n",
    "            range(self.voxel_bounds[0]),\n",
    "            range(self.voxel_bounds[1]),\n",
    "            range(self.voxel_bounds[2]),\n",
    "        )\n",
    "        self._voxel_coords = np.vstack((xv.flatten(), yv.flatten(), zv.flatten())).astype(int).T\n",
    "\n",
    "    def get_mesh(self):\n",
    "        \"\"\"\n",
    "        Reconstructs a 3D mesh from a tsdf volume using the marching cubes algorithm\n",
    "        Returns:\n",
    "            numpy.array [n, 3]: each row represents a 3D point.\n",
    "            numpy.array [k, 3]: each row is a list of point indices used to render triangles.\n",
    "            numpy.array [n, 3]: each row represents the normal vector for the corresponding 3D point.\n",
    "            numpy.array [n, 3]: each row represents the color of the corresponding 3D point.\n",
    "        \"\"\"\n",
    "        # Marching cubes\n",
    "        voxel_points, triangles, normals, _ = measure.marching_cubes(self._tsdf_volume, level=0, method='lewiner')\n",
    "        points_ind = np.round(voxel_points).astype(int)\n",
    "        points = self.voxel_to_world(self._volume_origin, voxel_points, self._voxel_size)\n",
    "        \n",
    "        # Get vertex colors\n",
    "        rgb_vals = self._color_volume[points_ind[:, 0], points_ind[:, 1], points_ind[:, 2]]\n",
    "        colors_r, colors_g, colors_b = rgb_vals[:, 0], rgb_vals[:, 1], rgb_vals[:, 2]\n",
    "        colors = np.floor(np.asarray([colors_r, colors_g, colors_b])).T\n",
    "        colors = colors.astype(np.uint8)\n",
    "        return points, triangles, normals, colors\n",
    "\n",
    "    def get_valid_points(self, depth_image, voxel_u, voxel_v, voxel_z):\n",
    "        \"\"\"Compute a boolean array for indexing the voxel volume. Note that every time the method integrate(...) is called, not every voxel in the volume will be updated. This method returns a boolean matrix called valid_points with dimension (n, ), where n = # of voxels. Index i of valid_points will be true if this voxel will be updated, false if the voxel needs not to be updated.\"\"\"\n",
    "        image_height, image_width = depth_image.shape\n",
    "        # Ensure voxel_u and voxel_v are valid integer indices\n",
    "        voxel_u = voxel_u.astype(int)\n",
    "        voxel_v = voxel_v.astype(int)\n",
    "    \n",
    "        # Eliminate pixels not in the image bounds or that are behind the image plane\n",
    "        valid_pixels = np.logical_and(voxel_u >= 0,\n",
    "                                      np.logical_and(voxel_u < image_width,\n",
    "                                      np.logical_and(voxel_v >= 0,\n",
    "                                      np.logical_and(voxel_v < image_height, voxel_z > 0))))\n",
    "    \n",
    "        # Get depths for valid coordinates u, v from the depth image. Zero elsewhere.\n",
    "        depths = np.zeros(voxel_u.shape)\n",
    "        depths[valid_pixels] = depth_image[voxel_v[valid_pixels], voxel_u[valid_pixels]]\n",
    "    \n",
    "        # Filter out zero depth values and depth + truncation margin >= voxel_z\n",
    "        valid_points = np.logical_and(depths > 0, depths+self._truncation_margin >= voxel_z)\n",
    "        return valid_points\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def voxel_to_world(volume_origin, voxel_size, voxel_coords):\n",
    "        \"\"\"Convert from voxel coordinates to world coordinates\"\"\"\n",
    "        return volume_origin + voxel_coords * voxel_size\n",
    "        \n",
    "    @staticmethod\n",
    "    def compute_tsdf(depth_image, voxel_z, truncation_margin, valid_points, valid_pixels):\n",
    "        \"\"\"Compute the new TSDF value for each valid point. We apply truncation and normalization in the end, so that tsdf value is in the range [-1,1].\"\"\"\n",
    "        Z = voxel_z[valid_points]\n",
    "        proj = np.zeros_like(Z)\n",
    "        \n",
    "        for idx, pixel in enumerate(valid_pixels):\n",
    "            u, v = pixel[0], pixel[1]\n",
    "            depth = depth_image[v, u]\n",
    "            proj[idx] = depth - Z[idx]\n",
    "            \n",
    "        for idx, dist in enumerate(proj):\n",
    "            proj[idx] = max(-1, min(1, dist/truncation_margin))\n",
    "\n",
    "        tsdf = proj\n",
    "        return tsdf\n",
    "\n",
    "    @staticmethod\n",
    "    def update_tsdf(tsdf_old, tsdf_new, color_old, color_new):\n",
    "        \"\"\"\n",
    "        Update the TSDF value and color for the voxels that have new observations. \n",
    "        We only update the tsdf and color value when the new absolute value of tsdf_new[i] is smaller than that of tsdf_old[i].\n",
    "        \"\"\"\n",
    "        for idx in range(len(tsdf_new)):\n",
    "            if abs(tsdf_new[idx]) < abs(tsdf_old[idx]):\n",
    "                tsdf_old[idx] = tsdf_new[idx]\n",
    "                color_old[idx, :] = color_new[idx, :]\n",
    "        return tsdf_old, color_old\n",
    "\n",
    "    def integrate(self, color_image, depth_image, camera_intrinsics, camera_pose):\n",
    "        \"\"\"\n",
    "        Integrate an RGB-D observation into the TSDF volume, by updating the tsdf volume, and color volume.\n",
    "        Args:\n",
    "            color_image (numpy.array [h, w, 3]): rgb image.\n",
    "            depth_image (numpy.array [h, w]): 'z' depth image.\n",
    "            camera_intrinsics (numpy.array [3, 3]): given as [[fu, 0, u0], [0, fv, v0], [0, 0, 1]]\n",
    "            camera_pose (numpy.array [4, 4]): SE3 transform representing pose (camera to world)\n",
    "        \"\"\"\n",
    "        color_image = color_image.astype(np.float32)\n",
    "        depth_image = depth_image.astype(np.float32)\n",
    "        voxel_world_coords = self.voxel_to_world(self._volume_origin, self._voxel_size, self._voxel_coords)\n",
    "        voxel_camera_coords = world_to_camera(camera_pose, voxel_world_coords)\n",
    "        voxel_img_coords = camera_to_image(camera_intrinsics, voxel_camera_coords)\n",
    "        voxel_u, voxel_v, voxel_z = voxel_img_coords[:,0], voxel_img_coords[:,1], voxel_camera_coords[:,2]\n",
    "        valid_points = self.get_valid_points(depth_image, voxel_u, voxel_v, voxel_z)\n",
    "        valid_voxels = self._voxel_coords[valid_points]\n",
    "        valid_pixels = voxel_img_coords[valid_points]\n",
    "        tsdf = self.compute_tsdf(depth_image, voxel_z, self._truncation_margin, valid_points, valid_pixels)\n",
    "\n",
    "        tsdf_old  = self._tsdf_volume[valid_voxels[:,0], valid_voxels[:,1], valid_voxels[:,2]]\n",
    "        color_old = self._color_volume[valid_voxels[:,0], valid_voxels[:,1], valid_voxels[:,2]]\n",
    "        color_new = color_image[valid_pixels[:,1].astype(int), valid_pixels[:,0].astype(int)]\n",
    "        \n",
    "        tsdf_updated, color_updated = self.update_tsdf(tsdf_old, tsdf, color_old, color_new)\n",
    "        self._tsdf_volume[valid_voxels[:,0], valid_voxels[:,1], valid_voxels[:,2]] = tsdf_updated\n",
    "        self._color_volume[valid_voxels[:,0], valid_voxels[:,1], valid_voxels[:,2]] = color_updated\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_count = 10\n",
    "    voxel_size = 0.01\n",
    "    volume_bounds = np.array([[-0.75, 0.75], [-0.75,0.75], [0.,0.8]])\n",
    "    camera_intrinsics = np.loadtxt(\"./data/camera-intrinsics.txt\")\n",
    "    tsdf_volume = TSDFVolume(volume_bounds, voxel_size)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(image_count):\n",
    "        print(f\"Fusing frame {i+1}/{image_count}\")\n",
    "    \n",
    "        # Load RGB-D image and camera pose\n",
    "        color_image = cv2.imread(f\"./data/frame-{i:06d}.color.png\")\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "        depth_image = cv2.imread(f\"./data/frame-{i:06d}.depth.png\", -1).astype(float) / 1000.0\n",
    "        camera_pose = np.loadtxt(f\"./data/frame-{i:06d}.pose.txt\")\n",
    "    \n",
    "        # Integrate observation into voxel volume\n",
    "        tsdf_volume.integrate(color_image, depth_image, camera_intrinsics, camera_pose)\n",
    "\n",
    "    fps = image_count / (time.time() - start_time)\n",
    "    print(\"Average FPS: {:.2f}\".format(fps))\n",
    "\n",
    "    # Get mesh from voxel volume and save to disk (can be viewed with Meshlab)\n",
    "    print(\"Saving mesh to mesh.ply...\")\n",
    "    points, faces, normals, colors = tsdf_volume.get_mesh()\n",
    "    mesh = Ply(triangles=faces, points=points, normals=normals, colors=colors)\n",
    "    mesh.write('mesh_output.ply')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405cbfe-39e6-47a1-acf8-af6778b66662",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f481c-f360-43df-b646-d8034c3a9a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec70ee-17e4-4163-b4b3-4d949232f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = np.array([1,2,2,0,1])\n",
    "filters = z > 0\n",
    "z[filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35e605-259c-4873-a1c7-6c7022ee2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05259ff9-008d-4ede-b0dd-48a7e195e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce72ea-caaa-4ae9-9303-df1203ce5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,3], \n",
    "              [3,4],\n",
    "              [5,6]])\n",
    "x[:,0].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec125d-c231-4bb5-b174-18995da83a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_coords_h = np.hstack((world_points, np.ones(len(world_points), 1)), np.float32())\n",
    "camera_coords_h = camera_pose @ homo_coords\n",
    "camera_coords_c = camera_coords_h[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b725f3-8781-43c3-b3d2-76a8f2bb66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds_points = np.array([[1,2,3], [1,2,3]])\n",
    "world_coords_h = np.hstack([worlds_points, np.ones((len(worlds_points), 1), np.float32)])\n",
    "world_coords_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58484a8b-066e-46fc-9aa1-d4c52cf8ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pose = np.random.rand(4,4)\n",
    "camera_coords_h = camera_pose @ world_coords_h.T\n",
    "camera_coords_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a56276-0260-4566-b244-6a9a84a791d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47207fcf-f67d-45b4-ac22-0e29f8234ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b95777-ed5f-422f-bee1-6927d56bd76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6739d-80df-451c-b2c0-04b319b68b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a108f-ecc2-435f-8b37-7fd3a89be27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt(\"./data/camera-intrinsics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998027b-1348-43d8-90bf-46152c0ac1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd88913-675e-4587-92be-f74a2eae4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bcebd-2405-4899-9a8c-48326bd87f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e6b77-523b-4a11-be53-8a537d246ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in os.listdir('./data'):\n",
    "    if filename.endswith(\".png\"):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b27c8-08c8-44e2-be81-8afbb87fef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img_color = cv2.imread('frame-000000.color.png',cv2.IMREAD_COLOR)\n",
    "img_color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e2ff5-9376-4f9d-89e4-e066afe01841",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c801c-4d03-47d1-8c5c-e9fccaac9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Ply(object):\n",
    "    \"\"\"Class to represent a ply in memory, read plys, and write plys.\n",
    "    \"\"\"\n",
    "    def __init__(self, ply_path=None, triangles=None, points=None, normals=None, colors=None):\n",
    "        \"\"\"Initialize the in memory ply representation.\n",
    "\n",
    "        Args:\n",
    "            ply_path (str, optional): Path to .ply file to read (note only\n",
    "                supports text mode, not binary mode). Defaults to None.\n",
    "            triangles (numpy.array [k, 3], optional): each row is a list of point indices used to\n",
    "                render triangles. Defaults to None.\n",
    "            points (numpy.array [n, 3], optional): each row represents a 3D point. Defaults to None.\n",
    "            normals (numpy.array [n, 3], optional): each row represents the normal vector for the\n",
    "                corresponding 3D point.. Defaults to None.\n",
    "            colors (numpy.array [n, 3], optional): each row represents the color of the\n",
    "                corresponding 3D point.. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # If ply path is None, load in triangles, point, normals, colors.\n",
    "        # else load ply from file. If ply_path is specified AND other inputs\n",
    "        # are specified as well, ignore other inputs.\n",
    "        # If normals are not None make sure that there are equal number of points and normals.\n",
    "        # If colors are not None make sure that there are equal number of colors and normals.\n",
    "\n",
    "        self.triangles = []\n",
    "        self.points = []\n",
    "        self.normals = []\n",
    "        self.colors = []\n",
    "\n",
    "        if ply_path is not None:\n",
    "            assert os.path.exists(ply_path)\n",
    "            self.read(ply_path)\n",
    "        else:\n",
    "            # Set from input args.\n",
    "            if triangles is not None:\n",
    "                self.triangles = triangles\n",
    "\n",
    "            if points is not None:\n",
    "                self.points = points\n",
    "\n",
    "            if normals is not None:\n",
    "                self.normals = normals\n",
    "\n",
    "            if colors is not None:\n",
    "                self.colors = colors\n",
    "\n",
    "        if len(self.normals) != 0:\n",
    "            assert len(self.normals) == len(self.points)\n",
    "\n",
    "        if len(self.colors) != 0:\n",
    "            assert len(self.colors) == len(self.points)\n",
    "\n",
    "\n",
    "    def write(self, ply_path):\n",
    "        \"\"\"Write mesh, point cloud, or oriented point cloud to ply file.\n",
    "\n",
    "        Args:\n",
    "            ply_path (str): Output ply path.\n",
    "        \"\"\"\n",
    "        with open(ply_path, 'w') as f:\n",
    "            # Write header.\n",
    "            f.write('ply\\n')\n",
    "            f.write('format ascii 1.0\\n')\n",
    "            f.write('element vertex {}\\n'.format(len(self.points)))\n",
    "            f.write('property float x\\n')\n",
    "            f.write('property float y\\n')\n",
    "            f.write('property float z\\n')\n",
    "\n",
    "            if len(self.normals) != 0:\n",
    "                f.write('property float nx\\n')\n",
    "                f.write('property float ny\\n')\n",
    "                f.write('property float nz\\n')\n",
    "\n",
    "            if len(self.colors) != 0:\n",
    "                f.write('property uchar red\\n')\n",
    "                f.write('property uchar green\\n')\n",
    "                f.write('property uchar blue\\n')\n",
    "\n",
    "            # Write faces header if dumping triangles.\n",
    "            if len(self.triangles) != 0:\n",
    "                f.write('element face {}\\n'.format(len(self.triangles)))\n",
    "                f.write('property list uchar int vertex_index\\n')\n",
    "\n",
    "            f.write('end_header\\n')\n",
    "\n",
    "            # Write points.\n",
    "            for i in range(len(self.points)):\n",
    "                f.write('{0} {1} {2}'.format(\n",
    "                    self.points[i][0],\n",
    "                    self.points[i][1],\n",
    "                    self.points[i][2]))\n",
    "\n",
    "                if len(self.normals) != 0:\n",
    "                    f.write(' {0} {1} {2}'.format(\n",
    "                        self.normals[i][0],\n",
    "                        self.normals[i][1],\n",
    "                        self.normals[i][2]))\n",
    "\n",
    "                if len(self.colors) != 0:\n",
    "                    f.write(' {0} {1} {2}'.format(\n",
    "                        int(self.colors[i][0]),\n",
    "                        int(self.colors[i][1]),\n",
    "                        int(self.colors[i][2])))\n",
    "\n",
    "                f.write('\\n')\n",
    "\n",
    "            # write triangles if they exist\n",
    "            for triangle in self.triangles:\n",
    "                f.write('3 {0} {1} {2}\\n'.format(triangle[0], triangle[1], triangle[2]))\n",
    "\n",
    "    def read(self, ply_path):\n",
    "        \"\"\"Read a ply into memory.\n",
    "\n",
    "        Args:\n",
    "            ply_path (str): ply to read in.\n",
    "        \"\"\"\n",
    "        vertex_mode = False\n",
    "        face_mode = False\n",
    "        num_points = 0\n",
    "        num_faces = 0\n",
    "        index = 0\n",
    "\n",
    "        self.points = []\n",
    "        self.normals = []\n",
    "        self.colors = []\n",
    "        self.triangles = []\n",
    "\n",
    "        parse_order = []\n",
    "\n",
    "        with open(ply_path, 'r') as ps:\n",
    "            for line in ps:\n",
    "                line = line.split()\n",
    "\n",
    "                if vertex_mode:\n",
    "                    # Read in points and normals.\n",
    "                    property_dict = {}\n",
    "\n",
    "                    assert len(parse_order) == len(line)\n",
    "\n",
    "                    for i, key in enumerate(parse_order):\n",
    "                        property_dict[key] = float(line[i])\n",
    "\n",
    "                    if ('x' in property_dict) and ('y' in property_dict) and ('z' in property_dict):\n",
    "                        self.points.append([property_dict['x'], property_dict['y'], property_dict['z']])\n",
    "                    if ('nx' in property_dict) and ('ny' in property_dict) and ('nz' in property_dict):\n",
    "                        self.normals.append([property_dict['nx'], property_dict['ny'], property_dict['nz']])\n",
    "                    if ('red' in property_dict) and ('green' in property_dict) and ('blue' in property_dict):\n",
    "                        self.colors.append([property_dict['red'], property_dict['green'], property_dict['blue']])\n",
    "                    index += 1\n",
    "                    if index == num_points:\n",
    "                        vertex_mode = False\n",
    "                        face_mode = True\n",
    "                        index = 0\n",
    "                elif face_mode:\n",
    "                    # Read in triangles.\n",
    "                    self.triangles.append([int(i) for i in line[1:4]])\n",
    "                    index += 1\n",
    "                    if index == num_faces:\n",
    "                        face_mode = False\n",
    "                elif line[0] == 'element':\n",
    "                    # set number of lines for vertices and faces.\n",
    "                    if line[1] == 'vertex':\n",
    "                        num_points = int(line[2])\n",
    "                    elif line[1] == 'face':\n",
    "                        num_faces = int(line[2])\n",
    "                elif line[0] == 'property' and num_faces <= 0:\n",
    "                    parse_order.append(line[2])\n",
    "                elif line[0] == 'end_header':\n",
    "                    vertex_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0099f-a25b-4e1a-8f49-0fa94ca5ec5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2c3f8-9dbf-41fc-8816-ff4c170f022a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ca198-028a-4409-a3d7-f0d328bb1b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101846d0-12d7-422d-8b09-8dd3f31c4d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcca87-687a-4dc8-86af-41a604402acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b17c6f-2ef3-4c5d-9a0c-44ff335888e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466b351-290f-421b-92f9-2fd78fb559af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0ae07-eecd-4dd2-b161-d9aa92b056c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3070801-8b45-4c1e-80c0-bc4df7c236c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5a110-1b64-4bc9-8cdf-a414f711b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1), nn.ReLU()),\n",
    "            nn.Sequential(nn.MaxPool2d(kernel_size=2), nn.Conv2d(16, 32, 3, padding=1), nn.ReLU()),\n",
    "            nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=1), nn.ReLU()),\n",
    "            nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(64, 128, 3, padding=1), nn.ReLU()),\n",
    "            nn.Sequential(nn.MaxPool2d(2), nn.Conv2d(128, 256, 3, padding=1), nn.ReLU()),\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(128+256, 128, 3, padding=1)),\n",
    "            nn.Sequential(nn.Conv2d(64+128, 64, 3, padding=1)),\n",
    "            nn.Sequential(nn.Conv2d(32+64, 32, 3, padding=1)),\n",
    "            nn.Sequential(nn.Conv2d(16+32, 16, 3, padding=1))\n",
    "        ])\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv1x1 = nn.Conv2d(16, 6, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for encoder in self.encoders[:-1]:\n",
    "            x = encoder(x)\n",
    "            skips.append(x)\n",
    "        x = self.encoders[-1](x)\n",
    "        for idx, decoder in enumerate(self.decoders):\n",
    "            x = self.upsample(x)\n",
    "            x = torch.cat([x, skips[-(idx+1)]], dim=1)\n",
    "            x = F.relu(decoder(x))\n",
    "        return self.conv1x1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469a7c2-5418-442e-9082-3fbc80ebc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniUnet()\n",
    "x = torch.rand(1,3,240,320)\n",
    "output = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2be14b-f17b-4ad5-96b5-9ea8894c4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048dc02-3c04-4337-bd71-accf54d6f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653beb6-5e7d-48f0-909a-fc46327f2fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd8257-1034-4264-bdb7-2d24ab7c5f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72cf63-fbbb-43c2-b79d-b0ba990806b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a95feb-ac20-499f-8fc3-fab612f528cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679247d5-98fb-440c-89f2-365a84377876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac962a89-00e3-42d2-bbe6-367da829e87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb03a5-b666-4444-b3db-8c804b4352fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df0819-867b-40f8-8d6a-2caded706a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37486c0b-374b-43e0-ad5f-3673f1bd8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasete.py\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class RGBDataset(Dataset):\n",
    "    def __init__(self, root_dir, ground_truth=False, transform=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.ground_truth = ground_truth\n",
    "        self.transform = transform\n",
    "        filenames = os.listdir(os.path.join(self.root_dir, 'rgb'))\n",
    "        self.dataset_len = len(filenames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_img = cv2.imread(os.path.join(self.root_dir, 'rgb', f\"{idx}_rgb.png\"))\n",
    "        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.ground_truth:\n",
    "            label = cv2.imread(os.path.join(self.root_dir, 'gt', f\"{idx}_gt.png\"))\n",
    "            sample = {'image':rgb_img, 'label':label}\n",
    "        else:\n",
    "            sample = {'image':rgb_img}\n",
    "\n",
    "        if self.transform:\n",
    "            mean_rgb = [0.722, 0.751, 0.807]\n",
    "            std_rgb = [0.171, 0.179, 0.197]\n",
    "            transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize(mean=mean_rgb, std=std_rgb),])\n",
    "            sample['image'] = transform(sample['image'])\n",
    "        return sample\n",
    "\n",
    "    def display_sample(self, idx):\n",
    "        sample = self.__getitem__(idx)\n",
    "        plt.imshow(sample['image'].permute(1,2,0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac8aa9-4b14-49c3-a3da-92b9ee70cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RGBDataset('/Users/rawri/Documents/GitHub/pose-estimation/dataset/train', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c6a7a-6ee8-43d1-b679-50c8f7bdd735",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37f902-7983-44e3-b613-3ff928afbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e42e0-c525-4cad-af1f-547b9759d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.display_sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d096de-e5f6-4413-810e-6ed47c3fb91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b509bf-98b5-4265-8daa-295a883071a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b330a8-c3c8-4db3-abf6-95235a2ec1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2daf245-2875-4230-ade7-3a060b1c932a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f2a4b-e12e-4196-bfee-f2fc34b9d7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f320338-e587-47b8-8780-7e0616fa95bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50054f-0419-4c36-9f43-cd705bc44274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204357c7-7e3a-4df6-8048-16511b293d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    select appropriate device\n",
    "    read dataset\n",
    "    transform dataset\n",
    "    create dataloaders \n",
    "    instantitate model\n",
    "    trainning loop\n",
    "        forward pass\n",
    "        loss : track train loss if using training data\n",
    "        compute gradients thru backward pass\n",
    "        perform optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c0a2b-20aa-409f-8f37-225515efead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def save_checkpoint(model, epoch, val_miou, path='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the model's state dictionary.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'model_miou': val_miou,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, path='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Load a saved model checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model, checkpoint['epoch'], checkpoint['model_miou']\n",
    "\n",
    "\n",
    "def plot_learning_curve(train_loss_list, train_miou_list, val_loss_list, val_miou_list, path='learning_curve.png'):\n",
    "    \"\"\"\n",
    "    Plot and save the training and validation loss/mIoU curves.\n",
    "    \"\"\"\n",
    "    epochs = np.arange(1, len(train_loss_list) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss_list, label=\"train_loss\", color='navy')\n",
    "    plt.plot(epochs, train_miou_list, label=\"train_mIoU\", color='teal')\n",
    "    plt.plot(epochs, val_loss_list, label=\"val_loss\", color='orange')\n",
    "    plt.plot(epochs, val_miou_list, label=\"val_mIoU\", color='gold')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mIoU / Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_predictions(model, device, dataloader, dataset_dir):\n",
    "    \"\"\"\n",
    "    Save model predictions to the specified directory.\n",
    "    \"\"\"\n",
    "    pred_dir = Path(dataset_dir) / 'pred'\n",
    "    pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Saving predicted masks to {pred_dir}\")\n",
    "\n",
    "    model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(dataloader):\n",
    "            inputs = batch['input'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            for i in range(preds.size(0)):\n",
    "                scene_id = batch_id * dataloader.batch_size + i\n",
    "                mask = preds[i].cpu().numpy()\n",
    "                mask_path = pred_dir / f\"{scene_id}_pred.png\"\n",
    "                show_mask(mask)\n",
    "                image.write_mask(mask, str(mask_path))\n",
    "\n",
    "\n",
    "def iou(prediction, target):\n",
    "    \"\"\"\n",
    "    Compute IoU for each class excluding the background.\n",
    "    \"\"\"\n",
    "    _, pred = torch.max(prediction, dim=1)\n",
    "    batch_size = prediction.size(0)\n",
    "    class_num = prediction.size(1)\n",
    "    batch_ious = []\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        class_ious = []\n",
    "        for class_id in range(1, class_num):  # Skip background class\n",
    "            mask_pred = (pred[batch_id] == class_id).int()\n",
    "            mask_target = (target[batch_id] == class_id).int()\n",
    "            if mask_target.sum() == 0:  # Skip if target is not present\n",
    "                continue\n",
    "            intersection = (mask_pred * mask_target).sum().item()\n",
    "            union = mask_pred.sum().item() + mask_target.sum().item() - intersection\n",
    "            class_ious.append(float(intersection) / float(union) if union > 0 else 0.0)\n",
    "        batch_ious.append(np.mean(class_ious) if class_ious else 0.0)\n",
    "    \n",
    "    return batch_ious\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de8aea-d70c-484c-b7f0-4ed030986491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1843bc-e320-40ba-96f0-beb653a385e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9778af-73cc-4683-a642-aafd7b51ce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fca472-1ab5-4ec0-98bd-56175566ad0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e0ce9-d8fd-4ff8-8184-b9d9681f1e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9185c5-08f6-48d7-b09d-e8ed5128a460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7051c9a-758e-431c-8026-a0543d835d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1bcfe-1143-4fd2-80f5-420681818590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e0095-b21e-486c-8f38-5de960bb8544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399f131-4749-40c0-a911-40fd55a4f98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113b13b-f0cd-4350-9344-ac2c9aae1817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca118db3-f8b2-4ab2-a153-20c247745ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import image\n",
    "from dataset import RGBDataset\n",
    "from model import MiniUNet\n",
    "from segmentation_helper import check_dataset, check_dataloader, show_mask\n",
    "\n",
    "def run_epoch(model, device, dataloader, criterion, optimizer=None):\n",
    "    \"\"\"\n",
    "    Run a single epoch of training or validation.\n",
    "    \"\"\"\n",
    "    is_training = optimizer is not None\n",
    "    model.train() if is_training else model.eval()\n",
    "\n",
    "    epoch_loss, epoch_iou, data_size = 0, 0, 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_iou += np.sum(iou(outputs, targets))\n",
    "        data_size += batch_size\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return epoch_loss / data_size, epoch_iou / data_size\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check for device (GPU/CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Directories setup\n",
    "    root_dir = Path('./dataset/')\n",
    "    train_dir = root_dir / 'train'\n",
    "    val_dir = root_dir / 'val'\n",
    "    test_dir = root_dir / 'test'\n",
    "\n",
    "    # Load datasets and dataloaders\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 2\n",
    "    train_dataset = RGBDataset(train_dir, has_gt=True)\n",
    "    val_dataset = RGBDataset(val_dir, has_gt=True)\n",
    "    test_dataset = RGBDataset(test_dir, has_gt=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = MiniUNet().to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training and validation loop\n",
    "    num_epochs = 15\n",
    "    best_miou = float('-inf')\n",
    "    history = {'train_loss': [], 'train_miou': [], 'val_loss': [], 'val_miou': []}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "\n",
    "        train_loss, train_miou = run_epoch(model, device, train_loader, criterion, optimizer)\n",
    "        val_loss, val_miou = run_epoch(model, device, val_loader, criterion)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_miou'].append(train_miou)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_miou'].append(val_miou)\n",
    "\n",
    "        print(f\"Train loss/mIoU: {train_loss:.2f}/{train_miou:.2f}\")\n",
    "        print(f\"Validation loss/mIoU: {val_loss:.2f}/{val_miou:.2f}\")\n",
    "\n",
    "        if val_miou > best_miou:\n",
    "            best_miou = val_miou\n",
    "            save_checkpoint(model, epoch, val_miou)\n",
    "\n",
    "    # Load the best model and make predictions\n",
    "    model, best_epoch, best_miou = load_checkpoint(model)\n",
    "    save_predictions(model, device, val_loader, val_dir)\n",
    "    save_predictions(model, device, test_loader, test_dir)\n",
    "\n",
    "    # Plot and save learning curves\n",
    "    plot_learning_curve(history['train_loss'], history['train_miou'], history['val_loss'], history['val_miou'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80047bc5-0eda-4747-a254-5cc2f830f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a2a8b2-cd5b-4729-ac70-befc5fc83a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
